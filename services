
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
*/

import { GoogleGenAI, GenerateContentResponse, Modality } from "@google/genai";
import { HistoricalScenario, Source } from "../types";
import { decodeBase64, decodeAudioData } from "../utils/audioUtils";

const FLASH_MODEL = 'gemini-3-flash-preview'; 
const TTS_MODEL = 'gemini-2.5-flash-preview-tts';
const IMAGE_MODEL = 'gemini-2.5-flash-image';

function extractJSON(text: string): any {
  let jsonString = text.trim().replace(/```json/gi, '').replace(/```/g, '');
  const firstOpen = jsonString.indexOf('{');
  const lastClose = jsonString.lastIndexOf('}');
  if (firstOpen === -1 || lastClose === -1) throw new Error("Error en la frecuencia temporal.");
  return JSON.parse(jsonString.substring(firstOpen, lastClose + 1));
}

export async function researchLocationAndLanguage(location: string, targetLanguage: string): Promise<HistoricalScenario> {
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const prompt = `
    ERES UN HISTORIADOR MULTILINGÜE. Investiga el lugar: "${location}".
    EL USUARIO QUIERE VIVIR ESTA EXPERIENCIA EN: ${targetLanguage}.

    1. DETECTA LA ÉPOCA MÁS RELEVANTE de este lugar automáticamente.
    
    REGLAS DE IDIOMA Y AUDIO:
    - TODO EL CONTENIDO (context, script text, translation) DEBE ESTAR EN ${targetLanguage}.
    - EXCEPCIÓN DE AUTENTICIDAD: Si el lugar es prehispánico (antes de 1521), los personajes pueden iniciar su diálogo con un saludo corto en su lengua originaria (ej. "Pialli", "B'a'ax ka wa'alik") pero el resto del texto DEBE continuar en ${targetLanguage} para que el usuario entienda.
    
    REGLAS DE GÉNERO Y VOZ:
    - Identifica a 2 protagonistas reales.
    - FEMENINO: Voz 'Kore' o 'Aoede'.
    - MASCULINO: Voz 'Charon' o 'Puck'.

    ESTRUCTURA JSON:
    {
      "context": "Contexto histórico en ${targetLanguage}.",
      "accentProfile": "Cómo debe sonar en ${targetLanguage} con acento mexicano histórico.",
      "characters": [
        {
          "name": "Nombre Real", 
          "gender": "male|female", 
          "voice": "Charon|Puck|Kore|Aoede", 
          "visualDescription": "Descripción visual para retrato",
          "bio": "Breve biografía en ${targetLanguage}"
        }
      ],
      "script": [
        {
          "speaker": "Nombre", 
          "text": "Diálogo principal en ${targetLanguage} (con toque ancestral si es prehispánico)", 
          "translation": "Explicación breve del matiz cultural en ${targetLanguage}"
        }
      ]
    }
  `;

  const response = await ai.models.generateContent({
    model: FLASH_MODEL,
    contents: prompt,
    config: { 
      tools: [{ googleSearch: {} }],
      responseMimeType: "application/json"
    }
  });

  const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks || [];
  const sources = groundingChunks
    .filter((c: any) => c.web?.uri)
    .map((c: any) => ({ title: c.web.title || "Fuente", uri: c.web.uri }))
    .slice(0, 3);

  const data = extractJSON(response.text);
  return { ...data, sources };
}

export async function generateDialogueAudio(scenario: HistoricalScenario): Promise<AudioBuffer> {
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const speakerVoiceConfigs = scenario.characters.slice(0, 2).map((char, i) => {
    let vName: any = char.voice.toLowerCase();
    if (char.gender === 'female' && (vName === 'charon' || vName === 'puck')) vName = 'kore';
    if (char.gender === 'male' && (vName === 'kore' || vName === 'aoede')) vName = 'charon';
    const voiceName = (vName as string).charAt(0).toUpperCase() + (vName as string).slice(1).toLowerCase();
    
    return {
      speaker: `Actor_${i}`,
      voiceConfig: { prebuiltVoiceConfig: { voiceName } }
    };
  });

  let dialogueText = "";
  scenario.script.forEach(line => {
    const charIdx = scenario.characters.findIndex(c => c.name === line.speaker);
    const speakerTag = `Actor_${charIdx === -1 ? 0 : charIdx}`;
    dialogueText += `${speakerTag}: ${line.text}\n`;
  });

  const response = await ai.models.generateContent({
    model: TTS_MODEL, 
    contents: [{ parts: [{ text: dialogueText }] }],
    config: {
      responseModalities: [Modality.AUDIO], 
      speechConfig: { multiSpeakerVoiceConfig: { speakerVoiceConfigs } }
    }
  });

  const base64 = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData)?.inlineData?.data;
  if (!base64) throw new Error("Audio no disponible.");
  
  const ctx = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
  const buffer = await decodeAudioData(decodeBase64(base64), ctx, 24000, 1);
  await ctx.close();
  return buffer;
}

export async function generateCharacterAvatar(description: string): Promise<string | null> {
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
  try {
    const response = await ai.models.generateContent({
      model: IMAGE_MODEL,
      contents: { parts: [{ text: `Authentic historical oil painting, detailed face, appropriate period clothing: ${description}` }] }
    });
    const part = response.candidates?.[0]?.content?.parts?.find(p => p.inlineData);
    return part ? `data:image/png;base64,${part.inlineData.data}` : null;
  } catch { return null; }
}
